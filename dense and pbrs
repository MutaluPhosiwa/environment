# Import Libraries
import numpy as np
import random
import matplotlib.pyplot as plt

# QLearningAgent Class
class QLearningAgent:
    def __init__(self, learning_rate=0.9, discount_factor=0.4, exploration_rate=0.995):
       # ef __init__(self, learning_rate=0.2, discount_factor=0.5, exploration_rate=1.0):
        self.q_table = {}
        self.learning_rate= learning_rate
        self.discount_factor=discount_factor
        self.exploration_rate= exploration_rate

    def choose_action(self, state):
        if random.random() < self.exploration_rate:
            return random.choice([0,1,2])  # Exploring the 3 actions defined
        else:
            return np.argmax(self.q_table.get(state, [0,0,0]))  # Exploitation

    def learn(self,state,action, reward,next_state):
        old_value =self.q_table.get(state,[0, 0, 0])[action]
        future_value=np.max(self.q_table.get(next_state, [0, 0, 0]))
        self.q_table[state]= self.q_table.get(state, [0, 0, 0])

        self.q_table[state][action]=old_value +self.learning_rate*(reward+self.discount_factor*future_value-old_value)
    def decay_exploration(self,decay_rate=0.995,min_rate=0.01):
        self.exploration_rate = max(self.exploration_rate * decay_rate, min_rate)
    


# MiniGridEnv Class
class MiniGridEnv:
    def __init__(self):
        self.battery_level= 5
        self.min_battery_level=1
        self.max_battery= 10
        self.demand =[0.6, 0.55, 0.55, 0.5, 0.5, 0.6, 0.7, 1.2, 1.25, 1.1, 0.8, 0.7, 0.65, 0.6, 0.55, 0.6, 0.9, 1.3, 1.35, 1.2, 1.1, 0.9, 0.8, 0.7]
        self.solar_input=[0, 0, 0, 0, 0, 0.0006, 0.1472, 0.3802, 0.5538, 0.6695, 0.7284, 0.7303, 0.6844, 0.5871, 0.4379, 0.2418, 0.0183, 0, 0, 0, 0, 0, 0, 0]
        self.total_steps=24
        self.current_step=0
        self.battery_cost_per_unit=0.0
        self.grid_cost_peak=4.50
        self.grid_cost_standard=2.91
        self.grid_cost_offpeak=1.50
        self.solar_cost_per_unit=0

    def reset(self): # repeat after failed attempt at the game
        self.battery_level=5
        self.current_step= 0
        return self._get_state()

    def reward_function(self,action, battery_charge, demand, solar):
        reward=0
        current_hour=self.current_step % 24

        # make dynamic costing
        if current_hour in range(6, 9) or current_hour in range(17, 20):# Determine grid cost based on Eskom's structure. got values from internet
            grid_cost = self.grid_cost_peak
        elif current_hour in range(9, 17) or current_hour in range(20, 22):
            grid_cost=self.grid_cost_standard
        else:
            grid_cost=self.grid_cost_offpeak
        #defining the action space 0 1 2
        if action ==0: #  Battery
            if demand<= battery_charge:
                battery_charge-=demand # appending
                reward+ self.battery_cost_per_unit*demand
            else:
                reward-=(demand-battery_charge)*grid_cost
                battery_charge = min(battery_charge +solar self.max_battery)#charging the battery

        elif action==1:  # Using Solar
            if solar>=demand:
                battery_charge = min(battery_charge + (solar - demand),self.max_battery)
                reward += (solar - demand) * self.solar_cost_per_unit
            else:
                unmet_demand = demand - solar
                if unmet_demand <= battery_charge:
                    battery_charge -= unmet_demand
                    reward += self.battery_cost_per_unit * unmet_demand
                else:
                    reward -= unmet_demand * grid_cost
                    battery_charge = min(battery_charge + solar, self.max_battery)

        elif action == 2:  # Use Grid
            battery_charge = min(battery_charge + solar, self.max_battery)
            reward -= demand * grid_cost

      ##  if battery_charge < self.min_battery_level:
            #penalty = (self.min_battery_level - battery_charge)  # No fixed multiplier, just the difference
            #reward -= penalty* grid_cost
        return reward, battery_charge

    def step(self, action):
        solar = self.solar_input[self.current_step % len(self.solar_input)]
        demand = self.demand[self.current_step % len(self.demand)]
        battery_charge = self.battery_level
        reward, battery_charge = self.reward_function(action, battery_charge, demand, solar)
        self.battery_level = battery_charge

        self.current_step += 1
        done = self.current_step >= self.total_steps
        return self._get_state(), reward, done

    def _get_state(self):
        return (max(min(self.battery_level, self.max_battery), 0),
                self.solar_input[self.current_step % len(self.solar_input)],
                self.demand[self.current_step % len(self.demand)])


# Function to train the dense agent
def train_dense(episodes):
    env = MiniGridEnv()
    agent = QLearningAgent()
    all_rewards = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.learn(state, action, reward, next_state)
            total_reward += reward
            state = next_state

        all_rewards.append(total_reward)
        agent.decay_exploration()

    return all_rewards

# MiniGridEnvDense Class (Updated with PBRS)
class MiniGridEnvDense:
    def __init__(self):
        self.battery_level = 5
        self.max_battery = 10
        self.min_battery_level = 1
        self.demand = [0.6, 0.55, 0.55, 0.5, 0.5, 0.6, 0.7, 1.2, 1.25, 1.1, 0.8, 0.7, 0.65, 0.6, 0.55, 0.6, 0.9, 1.3, 1.35, 1.2, 1.1, 0.9, 0.8, 0.7]
        self.solar_input = [0, 0, 0, 0, 0, 0.0006, 0.1472, 0.3802, 0.5538, 0.6695, 0.7284, 0.7303, 0.6844, 0.5871, 0.4379, 0.2418, 0.0183, 0, 0, 0, 0, 0, 0, 0]
        
        self.total_steps = 24
        self.current_step = 0
        self.battery_cost_per_unit = 0.00
        self.grid_cost_peak = 4.50
        self.grid_cost_standard = 2.91
        self.grid_cost_offpeak = 1.50
        self.solar_cost_per_unit = 0

    def reset(self):
        self.battery_level = 5
        self.current_step = 0
        return self._get_state()

    def potential_function(self, battery_charge, solar, demand):
        # Amount of demand met by grid
        grid_reliance = max(0, demand - (solar + battery_charge))  
        
        # Penalty for low battery level
        battery_health_penalty = max(0, self.min_battery_level - battery_charge)  
        
        # Positive reward for using solar
        solar_reward = solar if solar >= demand else 0
        
        # Total potential function
        return -grid_reliance - battery_health_penalty + solar_reward

    def reward_function(self, action, battery_charge, demand, solar):
        reward = 0
        current_hour = self.current_step % 24

        # Determine grid cost based on Eskom's time-of-use structure
        if current_hour in range(6, 9) or current_hour in range(17, 20):
            grid_cost = self.grid_cost_peak
        elif current_hour in range(9, 17) or current_hour in range(20, 22):
            grid_cost = self.grid_cost_standard
        else:
            grid_cost = self.grid_cost_offpeak

        if action == 0:  # Use Battery
            if demand <= battery_charge:
                battery_charge -= demand
                reward += self.battery_cost_per_unit * demand
            else:
                reward -= (demand - battery_charge) * grid_cost
                battery_charge = min(battery_charge + solar, self.max_battery)

        elif action == 1:  # Use Solar
            if solar >= demand:
                battery_charge = min(battery_charge + (solar - demand), self.max_battery)
                reward += (solar - demand) * self.solar_cost_per_unit
            else:
                unmet_demand = demand - solar
                if unmet_demand <= battery_charge:
                    battery_charge -= unmet_demand
                    reward += self.battery_cost_per_unit * unmet_demand
                else:
                    reward -= unmet_demand * grid_cost
                    battery_charge = min(battery_charge + solar, self.max_battery)

        elif action == 2:  # Use Grid
            battery_charge = min(battery_charge + solar, self.max_battery)
            reward -= demand * grid_cost

        # Apply Potential-Based Reward Shaping (PBRS)
        pb_reward = self.potential_function(battery_charge, solar, demand)
        reward += pb_reward * grid_cost  # Optional: Adjust scaling factor if needed

        return reward, battery_charge

    def step(self, action):
        solar = self.solar_input[self.current_step % len(self.solar_input)]
        demand = self.demand[self.current_step % len(self.demand)]
        battery_charge = self.battery_level
        reward, battery_charge = self.reward_function(action, battery_charge, demand, solar)
        self.battery_level = battery_charge

        self.current_step += 1
        done = self.current_step >= self.total_steps
        return self._get_state(), reward, done

    def _get_state(self):
        return (max(min(self.battery_level, self.max_battery), 0),
                self.solar_input[self.current_step % len(self.solar_input)],
                self.demand[self.current_step % len(self.demand)])


def train_pbrs(episodes):
    env = MiniGridEnvDense()
    agent = QLearningAgent()
    all_rewards = []
    initial_state = env.reset()

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        total_pbrs_reward = 0  # Separate variable for PBRS reward
        done = False

        while not done:
            # Extract relevant information from the state
            battery_state = state[0]
            solar_state = state[1]
            demand_state = state[2]

            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.learn(state, action, reward, next_state)

            total_reward += reward  # Main reward
            total_pbrs_reward += env.potential_function(state[0], solar_state, demand_state)  # Track PBRS reward
            state = next_state

            
        all_rewards.append(total_reward-total_pbrs_reward )  # Store only the main reward
        agent.decay_exploration()

    return all_rewards




